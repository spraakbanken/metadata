name:
  swe: 'Dependensparsningsmodell: Stanza'
  eng: 'Dependency parsing model: Stanza'
short_description:
  swe: Förtränade modeller för dependensparsning.
  eng: Pretrained models for dependency parsing.
type: model
trainingdata: false
unlisted: false
successors: []
language_codes:
  - swe
in_collections: []
downloads:
  - url: https://svn.spraakbanken.gu.se/sb-arkiv/pub/stanza/synt_stanza_eval.zip

    format: zip
    description: ''
    license: CC BY 4.0
  - url: https://svn.spraakbanken.gu.se/sb-arkiv/pub/stanza/synt_stanza_full2.zip

    format: zip
    description: ''
    license: CC BY 4.0
  - url: https://svn.spraakbanken.gu.se/sb-arkiv/pub/stanza/stanza_pretrain.zip

    format: zip
    description: ''
    license: CC BY 4.0
interfaces: []
contact_info:
  name: Markus Forsberg
  email: sb-info@svenska.gu.se
  affiliation:
    organization: Språkbanken
    email: sb-info@svenska.gu.se
description:
  swe: ''
  eng: |-
    <h2>Models</h2>

    <p>Stanza is currently the default annotation tool used by Sparv. We provide two models that enable dependency parsing of Swedish (in the <a href="https://svn.spraakdata.gu.se/sb-arkiv/pub/mamba.html">Mamba-Dep</a> format, the format of <a href="https://spraakbanken.gu.se/en/resources/talbanken">TalbankenSBX</a>).</p>

    <p><b><code>stanza_eval</code></b> is trained on Talbanken_SBX_train with as Talbanken_SBX_dev as dev set and evaluated using Talbanken_SBX_test.  The evaluation results are reported in the table below. The LAS (when trained with gold POS and MSD tags) is 84.48. We used the Word2Vec embeddings trained on the CONLL17 corpus (using Word2Vec trained on a Göteborgs-Posten corpus yields a very similar result of 84.43, see more about embeddings <a href="https://spraakbanken.gu.se/resurser/embeddings">here</a>).</p>

    <p><b><code>stanza_full</code></b> is trained on Talbanken_SBX_train + Talbanken_SBX_dev with Talbanken_SBX_test as dev set. We cannot evaluate the performance of this model, but we expect it to perform better than <code>stanza_eval</code>, or at least not worse.</p>

    <p> We updated the "pretrain" file in spring 2025. This was a minor format change.</p>


    <h3>Using the models on your own</h3>
    <p>Unzip the model you want to use and the "pretrain" file (which contains <a href="http://vectors.nlpl.eu/repository/#">word2vec</a> embeddings encoded in a format required by Stanza). Place the two .pt files in stanza/saved_models/depparse. Run <code>bash scripts/parse.sh UD_Swedish-Talbanken</code> to parse a test set using a pretrained model. The output file will be created in the stanza/corpora folder. If you use other treebank name than UD_Swedish-Talbanken, you would have to rename the model files. The script assumes that the POS tags are already present in the test set.</p>

    <h3>Training your own models</h3>
    <p>Unzip the model you want to use and the "pretrain" file (which contains <a href="http://vectors.nlpl.eu/repository/#">word2vec</a> embeddings encoded in a format required by Stanza). Follow the instructions provided by <a href="https://stanfordnlp.github.io/stanza/installation_usage.html">Stanza</a>. If you need a pretrained part-of-speech model, you will find it <a href="https://spraakbanken.gu.se/resurser/stanza_morph">here</a>. <p>
keywords: []
caveats:
  swe: ''
  eng: ''
other_references: []
updated: 2020-12-09
doi: 10.23695/wh3y-2y24
