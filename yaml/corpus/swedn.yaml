name:
  swe: SweDN 1.0
  eng: SweDN 1.0
short_description:
  swe: A Swedish text summarization corpus
  eng: A Swedish text summarization corpus
type: corpus
trainingdata: true
unlisted: false
successors: []
language_codes:
  - swe
in_collections:
  - superlim
downloads:
  - url: https://svn.spraakdata.gu.se/sb-arkiv/pub/swedn/1.0/swedn.zip
    type: an archive with the dataset in JSONL and TSV formats and the documentation sheet
    format: zip
    info: ''
    licence: CC BY 4.0
    restriction: attribution
interface: []
contact_info:
  name: Aleksandrs Berdicevskis
  email: aleksandrs.berdicevskis@gu.se
  affiliation:
    organisation: Språkbanken
    email: sb-info@svenska.gu.se
description:
  swe: ''
  eng: |-
    <table style="width:100%">
    <tr>
    <td>I. IDENTIFYING INFORMATION</td>
    <td>
    </td>
    </tr>
    <tr>
    <td>Title*</td>
    <td>SWE-DN
    </td>
    </tr>
    <tr>
    <td>Subtitle</td>
    <td>A Swedish text summarization corpus
    </td>
    </tr>
    <tr>
    <td>Created by*</td>
    <td>Julius Monsen (julius.monsen@liu.se), Arne Jönsson (arne.jonsson@liu.se)
    </td>
    </tr>
    <tr>
    <td>Publisher(s)*</td>
    <td>Linköping University
    </td>
    </tr>
    <tr>
    <td>Link(s) / permanent identifier(s)*</td>
    <td>https://spraakbanken.gu.se/resurser/superlim
    </td>
    </tr>
    <tr>
    <td>License(s)*</td>
    <td>CC BY 4.0
    </td>
    </tr>
    <tr>
    <td>Abstract*</td>
    <td>The SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure
    </td>
    </tr>
    <tr>
    <td>Funded by*</td>
    <td>SweClarin
    </td>
    </tr>
    <tr>
    <td>Cite as</td>
    <td>[1] A method for building non-English corpora for abstractive text summarization, Julius Monsen, Arne Jönsson, Proceedings of the CLARIN Annual Conference, 2021 
    </td>
    </tr>
    <tr>
    <td>Related datasets</td>
    <td>Similar to CNN/DailyMail; part of SuperLim 2.0 collection
    </td>
    </tr>
    <tr>
    <td></td>
    <td>
    </td>
    </tr>
    <tr>
    <td>II. USAGE</td>
    <td>
    </td>
    </tr>
    <tr>
    <td>Key applications</td>
    <td>Training text summarizers, both extractive and abstractive.
    </td>
    </tr>
    <tr>
    <td>Intended task(s)/usage(s)</td>
    <td>Given a text (article), provide its summary.
    </td>
    </tr>
    <tr>
    <td>Recommended evaluation measures</td>
    <td>Harmonic mean of Bleu and Rouge; Rouge, BERTScore, Coh-Metrix
    </td>
    </tr>
    <tr>
    <td>Dataset function(s)</td>
    <td>Model development
    </td>
    </tr>
    <tr>
    <td>Recommended split(s)</td>
    <td>"The articles in the dataset fall into five categories: domestic news, economy, sports, culture, other. The training set consists of the first three categories (78% of the dataset), the test set contains the fourth category (12%), the test set the fifth category (10%). The purpose is to have a cross-domain split which helps evaluate the model's ability to generalize to new data. The "other" category was chosen for the test set as the most diverse one (and presumably the most difficult)."
    </td>
    </tr>
    <tr>
    <td></td>
    <td>
    </td>
    </tr>
    <tr>
    <td>III. DATA</td>
    <td>
    </td>
    </tr>
    <tr>
    <td>Primary data*</td>
    <td>Text
    </td>
    </tr>
    <tr>
    <td>Language*</td>
    <td>Swedish
    </td>
    </tr>
    <tr>
    <td>Dataset in numbers*</td>
    <td>38,121 news articles with corresponding preambles
    </td>
    </tr>
    <tr>
    <td>Nature of the content*</td>
    <td>News texts
    </td>
    </tr>
    <tr>
    <td>Format*</td>
    <td>JSONL and TSV files with id, headline, summary, article and article category. An additional file with various statistics for each entry (including length measures, embedding similarity and article category) can be accessed at Språkbanken's website. The entries can be matched using the ids.
    </td>
    </tr>
    <tr>
    <td>Data source(s)*</td>
    <td>Dagens Nyheter news texts from 2000--2020
    </td>
    </tr>
    <tr>
    <td>Data collection method(s)*</td>
    <td>Received 1,936,576 news articles from Dagens Nyheter
    </td>
    </tr>
    <tr>
    <td>Data selection and filtering*</td>
    <td>Filtered to resemble the CNN/DailyMail dataset, see [1]
    </td>
    </tr>
    <tr>
    <td>Data preprocessing*</td>
    <td>See [1]
    </td>
    </tr>
    <tr>
    <td>Data labeling*</td>
    <td>None
    </td>
    </tr>
    <tr>
    <td>Annotator characteristics</td>
    <td>
    </td>
    </tr>
    <tr>
    <td></td>
    <td>
    </td>
    </tr>
    <tr>
    <td>IV. ETHICS AND CAVEATS</td>
    <td>
    </td>
    </tr>
    <tr>
    <td>Ethical considerations</td>
    <td>
    </td>
    </tr>
    <tr>
    <td>Things to watch out for</td>
    <td>
    </td>
    </tr>
    <tr>
    <td></td>
    <td>
    </td>
    </tr>
    <tr>
    <td>V. ABOUT DOCUMENTATION</td>
    <td>
    </td>
    </tr>
    <tr>
    <td>Data last updated*</td>
    <td>20221217, Julius Monsen
    </td>
    </tr>
    <tr>
    <td>Which changes have been made, compared to the previous version*</td>
    <td>First data release
    </td>
    </tr>
    <tr>
    <td>Access to previous versions</td>
    <td>
    </td>
    </tr>
    <tr>
    <td>This document created*</td>
    <td>20221206, Arne Jönsson
    </td>
    </tr>
    <tr>
    <td>This document last updated*</td>
    <td>20230203, Aleksandrs Berdicevskis
    </td>
    </tr>
    <tr>
    <td>Where to look for further details</td>
    <td>
    </td>
    </tr>
    <tr>
    <td>Documentation template version*</td>
    <td>v1.1
    </td>
    </tr>
    <tr>
    <td></td>
    <td>
    </td>
    </tr>
    <tr>
    <td>VI. OTHER</td>
    <td>
    </td>
    </tr>
    <tr>
    <td>Related projects</td>
    <td>
    </td>
    </tr>
    <tr>
    <td>References</td>
    <td>[1] A method for building non-English corpora for abstractive text summarization, Julius Monsen, Arne Jönsson, Proceedings of the CLARIN Annual Conference, 2021 
    </td>
    </tr>
    </table>
    