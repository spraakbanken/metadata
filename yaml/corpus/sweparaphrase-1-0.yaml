name:
  swe: SveParafras
  eng: SweParaphrase
short_description:
  swe: En delmängd av referensdatan för semantisk textjämförelse (STS Benchmark).
  eng: A subset of the Semantic Textual Similarity reference data (STS Benchmark).
type: corpus
trainingdata: true
unlisted: true
successors:
  - sweparaphrase
language_codes:
  - swe
size:
  Sentence pairs: 165
in_collections:
  - superlim-1-0
downloads:
  - url: https://svn.spraakbanken.gu.se/sb-arkiv/pub/sweparaphrase/sweparaphrase-dev-165.tsv

    format: tsv
    description: ''
    license: CC-BY-4.0
  - url: https://svn.spraakbanken.gu.se/sb-arkiv/pub/sweparaphrase/sweparaphrase_documentation.tsv

    format: tsv
    description: ''
    license: CC-BY-4.0
interfaces: []
contact_info:
  name: Dana Dannélls
  email: dana.dannells@svenska.gu.se
  affiliation:
    organization: Språkbanken Text
    email: sb-info@svenska.gu.se
description:
  swe: |-
    <table  style="width:100%">
      <tbody>
        <tr>
          <td>
            I. IDENTIFYING INFORMATION
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            Title*
          </td>
          <td>
         SweParaphrase v1.0
          </td>
        </tr>
        <tr>
          <td>
            Subtitle
          </td>
          <td>
         Sentence-level semantic similarity dataset (a subset of the Swedish STS Benchmark).
          </td>
        </tr>
        <tr>
          <td>
         Created by*
          </td>
          <td>
           Dana Dannélls (dana.dannells@svenska.gu.se)
          </td>
        </tr>
        <tr>
          <td>
          Publisher(s)*
          </td>
          <td>
        Språkbanken Text (sb-info@svenska.gu.se)
          </td>
        </tr>
        <tr>
          <td>
            Link(s) / permanent identifier(s)*
          </td>
          <td>
        https://spraakbanken.gu.se/en/resources/sweparaphrase
          </td>
        </tr>
        <tr>
          <td>
            License(s)*
          </td>
          <td>
            CC BY 4.0
          </td>
        </tr>
        <tr>
          <td>
            Abstract*
          </td>
          <td>
        SweParaphrase is a subset of the automatically translated Swedish Semantic Textual Similarity dataset (Isbister and Sahlgren, 2020). It consists of 165 manually corrected Swedish sentence pairs paired with the original English sentences and their similarity scores ranging between 0 (no meaning overlap) and 5 (meaning equivalence). These scores were taken from the English data, they were assigned by Crowdsourcing through Mechanical Turk. Each sentence pair belongs to one genre (e.g. news, forums or captions). The task is to determine how similar two sentences are.
          </td>
        </tr>
        <tr>
          <td>
            Funded by*
          </td>
          <td>
        Vinnova (grant no. 2020-02523)
          </td>
        </tr>
        <tr>
          <td>
            Cite as
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            Related datasets
          </td>
          <td>
            Part of the SuperLim collection <https://spraakbanken.gu.se/en/resources/superlim>. Created from the development version of the automatically translated Swedish STS Benchmark <a href="https://github.com/timpal0l/sts-benchmark-swedish">https://github.com/timpal0l/sts-benchmark-swedish</a>. The English source <a href="http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark">http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark</a>.
          </td>
        </tr>
        <tr>
          <td>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
        II. USAGE
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            Key applications
          </td>
          <td>
            Machine Translation, Question Answering, Information Retrieval, Text classification, Semantic parsing, Evaluation of language models.
          </td>
        </tr>
        <tr>
          <td>
        Intended task(s)/usage(s)
          </td>
          <td>
        Given two senetences determine how similar they are.
          </td>
        </tr>
        <tr>
          <td>
        Recommended evaluation measures
          </td>
          <td>
        Pearson correlation coefficient or alternative measures.
          </td>
        </tr>
        <tr>
          <td>
        Dataset function(s)
          </td>
          <td>
        Testing
          </td>
        </tr>
        <tr>
          <td>
            Recommended split(s)
          </td>
          <td>
        Test data only.
          </td>
        </tr>
        <tr>
          <td>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
        III. DATA
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
        Primary data*
          </td>
          <td>
            Text
          </td>
        </tr>
        <tr>
          <td>
        Language*
          </td>
          <td>
        Swedish
          </td>
        </tr>
        <tr>
          <td>
        Dataset in numbers*
          </td>
          <td>
        165 sentence pairs; 3 genres; 9 sources.
          </td>
        </tr>
        <tr>
          <td>
        Nature of the content*
          </td>
          <td>
        Each pair belongs to one genre (e.g. news, forums or captions) and is linked to a file from source (e.g. headlines, answers-forums, images). The English pairs from which the Swedish sentences were translated are also included.
          </td>
        </tr>
        <tr>
          <td>
            Format*
          </td>
          <td>
            <p>The downloadable 'sweparaphrase-dev-165.csv' file contains 8 tab-separated columns:
            <br>(1) Sentence ID from the automatically translated Swedish dataset;
            <br>(2) Genre from source (captions, news, forum);
            <br>(3) File from source (images, headlines, answers);
            <br>(4) and (5) manually corrected Swedish sentence pairs;
            <br>(6) Similarity score from source (based on the English sentence pairs done by Crowdsourcing through Mechanical Turk);
            <br>(7) and (8) English sentence pairs from source.</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data source(s)*</p>
          </td>
          <td>
            <p> The original STS benchmark comprises 8628 sentence pairs, collected from SemEval 2012 (task 6), 2014 (task 10), 2015 (task 2), 2016 (task 1), 2017 (task 1) and *SEM 2013.</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data collection method(s)*</p>
          </td>
          <td>
            <p>Isbister and Sahlgren, 2020 [1] translated the complete English STS-B <a href="http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark#Reference"> http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark#Reference</a>. The original English set is collected from datasets from the SemEval shared tasks. </p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data selection and filtering*</p>
          </td>
          <td>
            <p>This subset is taken from the automatically translated version of STS-B. First we focused only on the development version. Second, we selected only sentences which were deemed accurate translations.</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data preprocessing*</p>
          </td>
          <td>
            <p>English sentence pairs were tab-seperated. Large chunks of texts appearing after the full stop of the sentence were removed. Scores with decimals longer than 4 were shortened. </p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data labeling*</p>
          </td>
          <td>
            <p>No additional labeling was added. In the English version each sentence pair is annotated with a score (0-5). This annotation was done by Crowdsourcing through Mechanical Turk. Scores were assigned to the source English pairs. </p>
          </td>
        </tr>
        <tr>
          <td>
            Annotator characteristics
          </td>
          <td>
            <p>Native speaker of Swedish; fluent non-native speaker of Swedish.</p>
          </td>
        </tr>
        <tr>
          <td>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>IV. ETHICS AND CAVEATS</p>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>Ethical considerations</p>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>Things to watch out for</p>
          </td>
          <td>
            <p>The similarity scores are based on the English data and are not necessarily representative for the Swedish counter parts. </p>
          </td>
        </tr>
        <tr>
          <td>

          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>V. ABOUT DOCUMENTATION</p>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data last updated*</p>
          </td>
          <td>
            <p>2021-05-31, v1.0</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Which changes have been made, compared to the previous version*</p>
          </td>
          <td>
            <p>This is the first official version</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Access to previous versions</p>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>This document created*</p>
          </td>
          <td>
            <p>2021-05-31, Dana Dannélls</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>This document last updated*</p>
          </td>
          <td>
            <p>2021-06-07, Dana Dannélls</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Where to look for further details</p>
          </td>
          <td>
            <p>[1],[2],[3],[4]</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Documentation template version*</p>
          </td>
          <td>
            <p>v1.0</p>
          </td>
        </tr>
        <tr>
          <td>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>VI. OTHER</p>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>Related projects</p>
          </td>
          <td>
          <p> Language models for Swedish authorities <a href="https://www.vinnova.se/en/p/language-models-for-swedish-authorities/">Vinnova (grant no. 2019-02996) </a></p>
          </td>
        </tr>
        <tr>
          <td>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>References</p>
          </td>
          <td>
            <p>[1] Isbister, T. and Sahlgren, M. (2020): Why not simply translate? A first swedish evaluation benchmark for semantic similarity. <i>Proceedings of the Eighth Swedish Language Technology Conference (SLTC), University of Gothenburg. <a href="https://gubox.box.com/v/SLTC-2020-paper-15">https://gubox.box.com/v/SLTC-2020-paper-15</a></i>. The automatically translated dataset <a href="https://svn.spraakbanken.gu.se/sb-arkiv/pub/sweparaphrase/stsb-mt-sv.zip"> https://svn.spraakbanken.gu.se/sb-arkiv/pub/sweparaphrase/stsb-mt-sv.zip</a></p>
        <p>[2] Yvonne Adesam, Aleksandrs Berdicevskis, Felix Morger (2020): SwedishGLUE – Towards a Swedish Test Set for Evaluating Natural Language Understanding Models. University of Gothenburg.
         <a href="https://gupea.ub.gu.se/bitstream/2077/67179/1/gupea_2077_67179_1.pdf">https://gupea.ub.gu.se/bitstream/2077/67179/1/gupea_2077_67179_1.pdf</a> </p>
    <p>[3] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018):
    GLUE: A multi-task benchmark and analysis platform for natural language
    understanding. arXiv preprint arXiv:1804.07461. <a href="https://arxiv.org/pdf/1804.07461.pdf">https://arxiv.org/pdf/1804.07461.pdf</a></p>
            <p>[4] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia (2017):  Semeval-2017 task1: Semantic textual similarity-multilingual and cross-lingual focused  evaluation. In 11th International Workshop on Semantic Evaluations, 2017. <a href="https://www.aclweb.org/anthology/S17-2001.pdf">https://www.aclweb.org/anthology/S17-2001.pdf</a></p>
          </td>
        </tr>
      </tbody>
    </table>
  eng: |-
    <table  style="width:100%">
      <tbody>
        <tr>
          <td>
            I. IDENTIFYING INFORMATION
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            Title*
          </td>
          <td>
         SweParaphrase v1.0
          </td>
        </tr>
        <tr>
          <td>
            Subtitle
          </td>
          <td>
         Sentence-level semantic similarity dataset (a subset of the Swedish STS Benchmark).
          </td>
        </tr>
        <tr>
          <td>
         Created by*
          </td>
          <td>
           Dana Dannélls (dana.dannells@svenska.gu.se)
          </td>
        </tr>
        <tr>
          <td>
          Publisher(s)*
          </td>
          <td>
        Språkbanken Text (sb-info@svenska.gu.se)
          </td>
        </tr>
        <tr>
          <td>
            Link(s) / permanent identifier(s)*
          </td>
          <td>
        https://spraakbanken.gu.se/en/resources/sweparaphrase
          </td>
        </tr>
        <tr>
          <td>
            License(s)*
          </td>
          <td>
            CC BY 4.0
          </td>
        </tr>
        <tr>
          <td>
            Abstract*
          </td>
          <td>
        SweParaphrase is a subset of the automatically translated Swedish Semantic Textual Similarity dataset (Isbister and Sahlgren, 2020). It consists of 165 manually corrected Swedish sentence pairs paired with the original English sentences and their similarity scores ranging between 0 (no meaning overlap) and 5 (meaning equivalence). These scores were taken from the English data, they were assigned by Crowdsourcing through Mechanical Turk. Each sentence pair belongs to one genre (e.g. news, forums or captions). The task is to determine how similar two sentences are.
          </td>
        </tr>
        <tr>
          <td>
            Funded by*
          </td>
          <td>
        Vinnova (grant no. 2020-02523)
          </td>
        </tr>
        <tr>
          <td>
            Cite as
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            Related datasets
          </td>
          <td>
            Part of the SuperLim collection <https://spraakbanken.gu.se/en/resources/superlim>. Created from the development version of the automatically translated Swedish STS Benchmark <a href="https://github.com/timpal0l/sts-benchmark-swedish">https://github.com/timpal0l/sts-benchmark-swedish</a>. The English source <a href="http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark">http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark</a>.
          </td>
        </tr>
        <tr>
          <td>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
        II. USAGE
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            Key applications
          </td>
          <td>
            Machine Translation, Question Answering, Information Retrieval, Text classification, Semantic parsing, Evaluation of language models.
          </td>
        </tr>
        <tr>
          <td>
        Intended task(s)/usage(s)
          </td>
          <td>
        Given two senetences determine how similar they are.
          </td>
        </tr>
        <tr>
          <td>
        Recommended evaluation measures
          </td>
          <td>
        Pearson correlation coefficient or alternative measures.
          </td>
        </tr>
        <tr>
          <td>
        Dataset function(s)
          </td>
          <td>
        Testing
          </td>
        </tr>
        <tr>
          <td>
            Recommended split(s)
          </td>
          <td>
        Test data only.
          </td>
        </tr>
        <tr>
          <td>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
        III. DATA
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
        Primary data*
          </td>
          <td>
            Text
          </td>
        </tr>
        <tr>
          <td>
        Language*
          </td>
          <td>
        Swedish
          </td>
        </tr>
        <tr>
          <td>
        Dataset in numbers*
          </td>
          <td>
        165 sentence pairs; 3 genres; 9 sources.
          </td>
        </tr>
        <tr>
          <td>
        Nature of the content*
          </td>
          <td>
        Each pair belongs to one genre (e.g. news, forums or captions) and is linked to a file from source (e.g. headlines, answers-forums, images). The English pairs from which the Swedish sentences were translated are also included.
          </td>
        </tr>
        <tr>
          <td>
            Format*
          </td>
          <td>
            <p>The downloadable 'sweparaphrase-dev-165.csv' file contains 8 tab-separated columns:
            <br>(1) Sentence ID from the automatically translated Swedish dataset;
            <br>(2) Genre from source (captions, news, forum);
            <br>(3) File from source (images, headlines, answers);
            <br>(4) and (5) manually corrected Swedish sentence pairs;
            <br>(6) Similarity score from source (based on the English sentence pairs done by Crowdsourcing through Mechanical Turk);
            <br>(7) and (8) English sentence pairs from source.</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data source(s)*</p>
          </td>
          <td>
            <p> The original STS benchmark comprises 8628 sentence pairs, collected from SemEval 2012 (task 6), 2014 (task 10), 2015 (task 2), 2016 (task 1), 2017 (task 1) and *SEM 2013.</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data collection method(s)*</p>
          </td>
          <td>
            <p>Isbister and Sahlgren, 2020 [1] translated the complete English STS-B <a href="http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark#Reference"> http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark#Reference</a>. The original English set is collected from datasets from the SemEval shared tasks. </p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data selection and filtering*</p>
          </td>
          <td>
            <p>This subset is taken from the automatically translated version of STS-B. First we focused only on the development version. Second, we selected only sentences which were deemed accurate translations.</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data preprocessing*</p>
          </td>
          <td>
            <p>English sentence pairs were tab-seperated. Large chunks of texts appearing after the full stop of the sentence were removed. Scores with decimals longer than 4 were shortened. </p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data labeling*</p>
          </td>
          <td>
            <p>No additional labeling was added. In the English version each sentence pair is annotated with a score (0-5). This annotation was done by Crowdsourcing through Mechanical Turk. Scores were assigned to the source English pairs. </p>
          </td>
        </tr>
        <tr>
          <td>
            Annotator characteristics
          </td>
          <td>
            <p>Native speaker of Swedish; fluent non-native speaker of Swedish.</p>
          </td>
        </tr>
        <tr>
          <td>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>IV. ETHICS AND CAVEATS</p>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>Ethical considerations</p>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>Things to watch out for</p>
          </td>
          <td>
            <p>The similarity scores are based on the English data and are not necessarily representative for the Swedish counter parts. </p>
          </td>
        </tr>
        <tr>
          <td>

          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>V. ABOUT DOCUMENTATION</p>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>Data last updated*</p>
          </td>
          <td>
            <p>2021-05-31, v1.0</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Which changes have been made, compared to the previous version*</p>
          </td>
          <td>
            <p>This is the first official version</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Access to previous versions</p>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>This document created*</p>
          </td>
          <td>
            <p>2021-05-31, Dana Dannélls</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>This document last updated*</p>
          </td>
          <td>
            <p>2021-06-07, Dana Dannélls</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Where to look for further details</p>
          </td>
          <td>
            <p>[1],[2],[3],[4]</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Documentation template version*</p>
          </td>
          <td>
            <p>v1.0</p>
          </td>
        </tr>
        <tr>
          <td>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>VI. OTHER</p>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>Related projects</p>
          </td>
          <td>
          <p> Language models for Swedish authorities <a href="https://www.vinnova.se/en/p/language-models-for-swedish-authorities/">Vinnova (grant no. 2019-02996) </a></p>
          </td>
        </tr>
        <tr>
          <td>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>
            <p>References</p>
          </td>
          <td>
            <p>[1] Isbister, T. and Sahlgren, M. (2020): Why not simply translate? A first swedish evaluation benchmark for semantic similarity. <i>Proceedings of the Eighth Swedish Language Technology Conference (SLTC), University of Gothenburg. <a href="https://gubox.box.com/v/SLTC-2020-paper-15">https://gubox.box.com/v/SLTC-2020-paper-15</a></i>. The automatically translated dataset <a href="https://svn.spraakbanken.gu.se/sb-arkiv/pub/sweparaphrase/stsb-mt-sv.zip"> https://svn.spraakbanken.gu.se/sb-arkiv/pub/sweparaphrase/stsb-mt-sv.zip</a></p>
        <p>[2] Yvonne Adesam, Aleksandrs Berdicevskis, Felix Morger (2020): SwedishGLUE – Towards a Swedish Test Set for Evaluating Natural Language Understanding Models. University of Gothenburg.
         <a href="https://gupea.ub.gu.se/bitstream/2077/67179/1/gupea_2077_67179_1.pdf">https://gupea.ub.gu.se/bitstream/2077/67179/1/gupea_2077_67179_1.pdf</a> </p>
    <p>[3] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018):
    GLUE: A multi-task benchmark and analysis platform for natural language
    understanding. arXiv preprint arXiv:1804.07461. <a href="https://arxiv.org/pdf/1804.07461.pdf">https://arxiv.org/pdf/1804.07461.pdf</a></p>
            <p>[4] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia (2017):  Semeval-2017 task1: Semantic textual similarity-multilingual and cross-lingual focused  evaluation. In 11th International Workshop on Semantic Evaluations, 2017. <a href="https://www.aclweb.org/anthology/S17-2001.pdf">https://www.aclweb.org/anthology/S17-2001.pdf</a></p>
          </td>
        </tr>
      </tbody>
    </table>
updated: 2022-03-16
doi: 10.23695/6t6h-ss96
