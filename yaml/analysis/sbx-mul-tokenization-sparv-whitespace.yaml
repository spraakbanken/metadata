# This file was automatically generated by Sparv. Do not make changes directly to this file as they will get overwritten.
standard_reference: Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language
  Processing with Python. O’Reilly Media Inc.
tool:
  name: NLTK
  url: https://www.nltk.org/
description:
  swe: Dokumentationen för NLTKs RegexpTokenizer finns [här](https://www.nltk.org/api/nltk.tokenize.regexp.html#nltk.tokenize.regexp.RegexpTokenizer).
  eng: The documentation for NLTK's RegexpTokenizer can be found [here](https://www.nltk.org/api/nltk.tokenize.regexp.html#nltk.tokenize.regexp.RegexpTokenizer).
created: 2010-12-15
updated: 2021-05-07
name:
  swe: Blankteckentokenisering
  eng: Blankspace tokenization
short_description:
  swe: Tokeniserar text utifrån blanktecken med hjälp av NLTKs RegexpTokenizer
  eng: Tokenizes text into tokens by whitespaces using the RegexpTokenizer from NLTK
task: tokenization
type: analysis
analysis_unit: token
example: |-
  This analysis is used with Sparv. Check out [Sparv's quick start guide](https://spraakbanken.gu.se/sparv/#/user-manual/quick-start) to get started!

  To use this analysis, add the following line under `export.annotations` in the Sparv [corpus configuration file](https://spraakbanken.gu.se/sparv/#/user-manual/quick-start?id=creating-the-config-file):

  ```yaml
  - segment.token  # Token segments
  ```

  In order to use this tokenizer you need to add the following setting to your Sparv corpus configuration file:
  ```yaml
  segment:
    token_segmenter: whitespace
  ```

  For more info on how to use Sparv, check out the [Sparv documentation](https://spraakbanken.gu.se/sparv).

  Example output:
  ```xml
  <token>Det</token>
  <token>här</token>
  <token>är</token>
  <token>en</token>
  <token>korpus</token>
  <token>.</token>
  ```
licences:
  code: MIT License
contact_info:
  name: Markus Forsberg
  email: sb-info@svenska.gu.se
  affiliation:
    organisation: Språkbanken Text
    email: sb-info@svenska.gu.se
